[2022-04-12 07:30:51.294203] batch_size ......... 128
[2022-04-12 07:30:51.294223] beam_widths ........ 301000
[2022-04-12 07:30:51.294226] checkpoint_path .... models/shakespeare/
[2022-04-12 07:30:51.294242] config ............. {'seed': 1234321, 'dont_print_args': False, 'cuda': True, 'device_num': 3, 'train_data_pct': 0.9, 'val_data_pct': 0.05, 'seq_len': 100, 'do_test': False, 'rnn_type': 'LSTM', 'hidden_size': 128, 'dropout': 0.3, 'num_layers': 2, 'masked_lm': False, 'checkpoint_path': 'models/shakespeare/', 'vocab_size': 68, 'data_path': 'data/shakespeare_input.txt', 'batch_size': 128, 'disable_tqdm': True, 'sample_type': 'beam_search', 'beam_widths': 301000, 'sample_args': {'coverage_type': 'fixed_width'}, 'num_seqs': 100000, 'total_seq_lens': 20, 'hist_len': 17, 'excluded': [1]}
[2022-04-12 07:30:51.294246] cuda ............... True
[2022-04-12 07:30:51.294249] data_path .......... data/shakespeare_input.txt
[2022-04-12 07:30:51.294252] device_num ......... 3
[2022-04-12 07:30:51.294255] disable_tqdm ....... True
[2022-04-12 07:30:51.294257] do_test ............ False
[2022-04-12 07:30:51.294260] dont_print_args .... False
[2022-04-12 07:30:51.294263] dont_shuffle ....... False
[2022-04-12 07:30:51.294266] dropout ............ 0.3
[2022-04-12 07:30:51.294270] excluded ........... [1]
[2022-04-12 07:30:51.294273] finetune ........... False
[2022-04-12 07:30:51.294276] grad_clip .......... 1.0
[2022-04-12 07:30:51.294279] hidden_size ........ 128
[2022-04-12 07:30:51.294282] hist_len ........... 17
[2022-04-12 07:30:51.294285] log_interval ....... 100
[2022-04-12 07:30:51.294288] lr ................. 0.001
[2022-04-12 07:30:51.294291] lr_decay_style ..... constant
[2022-04-12 07:30:51.294294] masked_lm .......... False
[2022-04-12 07:30:51.294297] num_layers ......... 2
[2022-04-12 07:30:51.294300] num_seqs ........... 100000
[2022-04-12 07:30:51.294302] num_workers ........ 0
[2022-04-12 07:30:51.294305] optimizer .......... adam
[2022-04-12 07:30:51.294308] rnn_type ........... LSTM
[2022-04-12 07:30:51.294311] sample_args ........ {'coverage_type': 'fixed_width'}
[2022-04-12 07:30:51.294314] sample_occ_range ... 4
[2022-04-12 07:30:51.294317] sample_type ........ beam_search
[2022-04-12 07:30:51.294320] save_epochs ........ 1
[2022-04-12 07:30:51.294323] seed ............... 1234321
[2022-04-12 07:30:51.294325] seq_len ............ 100
[2022-04-12 07:30:51.294328] shuffle ............ True
[2022-04-12 07:30:51.294331] total_seq_lens ..... 20
[2022-04-12 07:30:51.294334] train_data_pct ..... 0.9
[2022-04-12 07:30:51.294337] train_epochs ....... 40
[2022-04-12 07:30:51.294340] val_data_pct ....... 0.05
[2022-04-12 07:30:51.294343] valid_data_pct ..... 0.05
[2022-04-12 07:30:51.294346] vocab_size ......... 68
[2022-04-12 07:30:51.294349] warmup_pct ......... 0.01
[2022-04-12 07:30:51.294352] weight_decay ....... 0.0
==================================================
{'\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, "'": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '<BOS>': 12, '?': 13, 'A': 14, 'B': 15, 'C': 16, 'D': 17, 'E': 18, 'F': 19, 'G': 20, 'H': 21, 'I': 22, 'J': 23, 'K': 24, 'L': 25, 'M': 26, 'N': 27, 'O': 28, 'P': 29, 'Q': 30, 'R': 31, 'S': 32, 'T': 33, 'U': 34, 'V': 35, 'W': 36, 'X': 37, 'Y': 38, 'Z': 39, '[': 40, ']': 41, 'a': 42, 'b': 43, 'c': 44, 'd': 45, 'e': 46, 'f': 47, 'g': 48, 'h': 49, 'i': 50, 'j': 51, 'k': 52, 'l': 53, 'm': 54, 'n': 55, 'o': 56, 'p': 57, 'q': 58, 'r': 59, 's': 60, 't': 61, 'u': 62, 'v': 63, 'w': 64, 'x': 65, 'y': 66, 'z': 67}
[2022-04-12 07:30:54.285364] Checkpoint path [models/shakespeare/] does exist.
[2022-04-12 07:30:54.288197] Loaded model from models/shakespeare/shakespeare_model.pt
Getting samples from batch
  0%|          | 0/18 [00:00<?, ?it/s]/home/showalte/research/prob_seq_queries/seq_queries/sample.py:869: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  seq_inds = tokens // vocab_size
  6%|▌         | 1/18 [05:34<1:34:42, 334.28s/it] 11%|█         | 2/18 [11:27<1:32:03, 345.23s/it] 17%|█▋        | 3/18 [18:01<1:31:55, 367.73s/it] 22%|██▏       | 4/18 [24:33<1:28:02, 377.34s/it] 28%|██▊       | 5/18 [31:04<1:22:50, 382.34s/it] 33%|███▎      | 6/18 [37:38<1:17:13, 386.09s/it] 39%|███▉      | 7/18 [44:12<1:11:17, 388.87s/it] 44%|████▍     | 8/18 [51:00<1:05:49, 394.95s/it] 50%|█████     | 9/18 [58:22<1:01:25, 409.53s/it] 56%|█████▌    | 10/18 [1:05:24<55:07, 413.46s/it] 61%|██████    | 11/18 [1:12:01<47:37, 408.22s/it] 67%|██████▋   | 12/18 [1:18:31<40:17, 402.88s/it] 72%|███████▏  | 13/18 [1:25:16<33:36, 403.33s/it] 78%|███████▊  | 14/18 [1:32:11<27:08, 407.04s/it] 83%|████████▎ | 15/18 [1:39:05<20:27, 409.03s/it] 89%|████████▉ | 16/18 [1:45:58<13:40, 410.22s/it] 94%|█████████▍| 17/18 [1:52:31<06:45, 405.19s/it]100%|██████████| 18/18 [1:58:08<00:00, 384.66s/it]100%|██████████| 18/18 [1:58:08<00:00, 393.82s/it]
