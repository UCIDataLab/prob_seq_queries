[2022-04-12 13:14:56.078682] batch_size ......... 128
[2022-04-12 13:14:56.078712] beam_widths ........ 20160000
[2022-04-12 13:14:56.078719] checkpoint_path .... models/shakespeare/
[2022-04-12 13:14:56.078743] config ............. {'seed': 1234321, 'dont_print_args': False, 'cuda': True, 'device_num': 0, 'train_data_pct': 0.9, 'val_data_pct': 0.05, 'seq_len': 100, 'do_test': False, 'rnn_type': 'LSTM', 'hidden_size': 128, 'dropout': 0.3, 'num_layers': 2, 'masked_lm': False, 'checkpoint_path': 'models/shakespeare/', 'vocab_size': 68, 'data_path': 'data/shakespeare_input.txt', 'batch_size': 128, 'disable_tqdm': True, 'sample_type': 'beam_search', 'beam_widths': 20160000, 'sample_args': {'coverage_type': 'fixed_width'}, 'num_seqs': 100000, 'total_seq_lens': 20, 'hist_len': 16, 'excluded': [1]}
[2022-04-12 13:14:56.078750] cuda ............... True
[2022-04-12 13:14:56.078756] data_path .......... data/shakespeare_input.txt
[2022-04-12 13:14:56.078761] device_num ......... 0
[2022-04-12 13:14:56.078767] disable_tqdm ....... True
[2022-04-12 13:14:56.078773] do_test ............ False
[2022-04-12 13:14:56.078778] dont_print_args .... False
[2022-04-12 13:14:56.078784] dont_shuffle ....... False
[2022-04-12 13:14:56.078790] dropout ............ 0.3
[2022-04-12 13:14:56.078797] excluded ........... [1]
[2022-04-12 13:14:56.078802] finetune ........... False
[2022-04-12 13:14:56.078809] grad_clip .......... 1.0
[2022-04-12 13:14:56.078814] hidden_size ........ 128
[2022-04-12 13:14:56.078820] hist_len ........... 16
[2022-04-12 13:14:56.078825] log_interval ....... 100
[2022-04-12 13:14:56.078832] lr ................. 0.001
[2022-04-12 13:14:56.078838] lr_decay_style ..... constant
[2022-04-12 13:14:56.078844] masked_lm .......... False
[2022-04-12 13:14:56.078849] num_layers ......... 2
[2022-04-12 13:14:56.078855] num_seqs ........... 100000
[2022-04-12 13:14:56.078861] num_workers ........ 0
[2022-04-12 13:14:56.078866] optimizer .......... adam
[2022-04-12 13:14:56.078871] rnn_type ........... LSTM
[2022-04-12 13:14:56.078878] sample_args ........ {'coverage_type': 'fixed_width'}
[2022-04-12 13:14:56.078883] sample_occ_range ... 4
[2022-04-12 13:14:56.078889] sample_type ........ beam_search
[2022-04-12 13:14:56.078895] save_epochs ........ 1
[2022-04-12 13:14:56.078900] seed ............... 1234321
[2022-04-12 13:14:56.078906] seq_len ............ 100
[2022-04-12 13:14:56.078911] shuffle ............ True
[2022-04-12 13:14:56.078917] total_seq_lens ..... 20
[2022-04-12 13:14:56.078923] train_data_pct ..... 0.9
[2022-04-12 13:14:56.078929] train_epochs ....... 40
[2022-04-12 13:14:56.078935] val_data_pct ....... 0.05
[2022-04-12 13:14:56.078941] valid_data_pct ..... 0.05
[2022-04-12 13:14:56.078947] vocab_size ......... 68
[2022-04-12 13:14:56.078953] warmup_pct ......... 0.01
[2022-04-12 13:14:56.078959] weight_decay ....... 0.0
==================================================
{'\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, "'": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '<BOS>': 12, '?': 13, 'A': 14, 'B': 15, 'C': 16, 'D': 17, 'E': 18, 'F': 19, 'G': 20, 'H': 21, 'I': 22, 'J': 23, 'K': 24, 'L': 25, 'M': 26, 'N': 27, 'O': 28, 'P': 29, 'Q': 30, 'R': 31, 'S': 32, 'T': 33, 'U': 34, 'V': 35, 'W': 36, 'X': 37, 'Y': 38, 'Z': 39, '[': 40, ']': 41, 'a': 42, 'b': 43, 'c': 44, 'd': 45, 'e': 46, 'f': 47, 'g': 48, 'h': 49, 'i': 50, 'j': 51, 'k': 52, 'l': 53, 'm': 54, 'n': 55, 'o': 56, 'p': 57, 'q': 58, 'r': 59, 's': 60, 't': 61, 'u': 62, 'v': 63, 'w': 64, 'x': 65, 'y': 66, 'z': 67}
[2022-04-12 13:14:59.299835] Checkpoint path [models/shakespeare/] does exist.
[2022-04-12 13:14:59.302402] Loaded model from models/shakespeare/shakespeare_model.pt
Getting samples from batch
  0%|          | 0/18 [00:00<?, ?it/s]............./home/showalte/research/prob_seq_queries/seq_queries/sample.py:870: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  seq_inds = tokens // vocab_size
  6%|▌         | 1/18 [7:14:11<123:01:13, 26051.41s/it]............. 11%|█         | 2/18 [13:51:59<110:04:37, 24767.32s/it]............. 17%|█▋        | 3/18 [20:38:01<102:25:31, 24582.10s/it]............. 22%|██▏       | 4/18 [27:36:00<96:21:33, 24778.09s/it] ............. 28%|██▊       | 5/18 [34:23:45<89:04:07, 24665.20s/it]............. 33%|███▎      | 6/18 [41:11:41<82:00:10, 24600.85s/it].....