[2022-04-12 12:53:14.333985] batch_size ......... 128
[2022-04-12 12:53:14.334004] beam_widths ........ 20160000
[2022-04-12 12:53:14.334008] checkpoint_path .... models/shakespeare/
[2022-04-12 12:53:14.334024] config ............. {'seed': 1234321, 'dont_print_args': False, 'cuda': True, 'device_num': 0, 'train_data_pct': 0.9, 'val_data_pct': 0.05, 'seq_len': 100, 'do_test': False, 'rnn_type': 'LSTM', 'hidden_size': 128, 'dropout': 0.3, 'num_layers': 2, 'masked_lm': False, 'checkpoint_path': 'models/shakespeare/', 'vocab_size': 68, 'data_path': 'data/shakespeare_input.txt', 'batch_size': 128, 'disable_tqdm': True, 'sample_type': 'beam_search', 'beam_widths': 20160000, 'sample_args': {'coverage_type': 'fixed_width'}, 'num_seqs': 100000, 'total_seq_lens': 20, 'hist_len': 18, 'excluded': [1]}
[2022-04-12 12:53:14.334028] cuda ............... True
[2022-04-12 12:53:14.334031] data_path .......... data/shakespeare_input.txt
[2022-04-12 12:53:14.334034] device_num ......... 0
[2022-04-12 12:53:14.334037] disable_tqdm ....... True
[2022-04-12 12:53:14.334040] do_test ............ False
[2022-04-12 12:53:14.334043] dont_print_args .... False
[2022-04-12 12:53:14.334046] dont_shuffle ....... False
[2022-04-12 12:53:14.334049] dropout ............ 0.3
[2022-04-12 12:53:14.334052] excluded ........... [1]
[2022-04-12 12:53:14.334056] finetune ........... False
[2022-04-12 12:53:14.334059] grad_clip .......... 1.0
[2022-04-12 12:53:14.334062] hidden_size ........ 128
[2022-04-12 12:53:14.334065] hist_len ........... 18
[2022-04-12 12:53:14.334068] log_interval ....... 100
[2022-04-12 12:53:14.334071] lr ................. 0.001
[2022-04-12 12:53:14.334075] lr_decay_style ..... constant
[2022-04-12 12:53:14.334078] masked_lm .......... False
[2022-04-12 12:53:14.334080] num_layers ......... 2
[2022-04-12 12:53:14.334083] num_seqs ........... 100000
[2022-04-12 12:53:14.334086] num_workers ........ 0
[2022-04-12 12:53:14.334089] optimizer .......... adam
[2022-04-12 12:53:14.334092] rnn_type ........... LSTM
[2022-04-12 12:53:14.334096] sample_args ........ {'coverage_type': 'fixed_width'}
[2022-04-12 12:53:14.334099] sample_occ_range ... 4
[2022-04-12 12:53:14.334102] sample_type ........ beam_search
[2022-04-12 12:53:14.334105] save_epochs ........ 1
[2022-04-12 12:53:14.334108] seed ............... 1234321
[2022-04-12 12:53:14.334111] seq_len ............ 100
[2022-04-12 12:53:14.334113] shuffle ............ True
[2022-04-12 12:53:14.334116] total_seq_lens ..... 20
[2022-04-12 12:53:14.334120] train_data_pct ..... 0.9
[2022-04-12 12:53:14.334122] train_epochs ....... 40
[2022-04-12 12:53:14.334126] val_data_pct ....... 0.05
[2022-04-12 12:53:14.334129] valid_data_pct ..... 0.05
[2022-04-12 12:53:14.334132] vocab_size ......... 68
[2022-04-12 12:53:14.334135] warmup_pct ......... 0.01
[2022-04-12 12:53:14.334138] weight_decay ....... 0.0
==================================================
{'\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, "'": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '<BOS>': 12, '?': 13, 'A': 14, 'B': 15, 'C': 16, 'D': 17, 'E': 18, 'F': 19, 'G': 20, 'H': 21, 'I': 22, 'J': 23, 'K': 24, 'L': 25, 'M': 26, 'N': 27, 'O': 28, 'P': 29, 'Q': 30, 'R': 31, 'S': 32, 'T': 33, 'U': 34, 'V': 35, 'W': 36, 'X': 37, 'Y': 38, 'Z': 39, '[': 40, ']': 41, 'a': 42, 'b': 43, 'c': 44, 'd': 45, 'e': 46, 'f': 47, 'g': 48, 'h': 49, 'i': 50, 'j': 51, 'k': 52, 'l': 53, 'm': 54, 'n': 55, 'o': 56, 'p': 57, 'q': 58, 'r': 59, 's': 60, 't': 61, 'u': 62, 'v': 63, 'w': 64, 'x': 65, 'y': 66, 'z': 67}
[2022-04-12 12:53:17.592380] Checkpoint path [models/shakespeare/] does exist.
[2022-04-12 12:53:17.594614] Loaded model from models/shakespeare/shakespeare_model.pt
Getting samples from batch
  0%|          | 0/18 [00:00<?, ?it/s]/home/showalte/research/prob_seq_queries/seq_queries/sample.py:870: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  seq_inds = tokens // vocab_size
  6%|▌         | 1/18 [00:05<01:41,  5.95s/it] 11%|█         | 2/18 [00:11<01:30,  5.66s/it] 17%|█▋        | 3/18 [00:16<01:23,  5.58s/it] 22%|██▏       | 4/18 [00:22<01:17,  5.55s/it] 28%|██▊       | 5/18 [00:27<01:12,  5.54s/it] 33%|███▎      | 6/18 [00:33<01:08,  5.72s/it] 39%|███▉      | 7/18 [00:39<01:02,  5.64s/it] 44%|████▍     | 8/18 [00:44<00:55,  5.60s/it] 50%|█████     | 9/18 [00:50<00:50,  5.57s/it] 56%|█████▌    | 10/18 [00:55<00:44,  5.54s/it] 61%|██████    | 11/18 [01:01<00:38,  5.52s/it] 67%|██████▋   | 12/18 [01:06<00:33,  5.52s/it] 72%|███████▏  | 13/18 [01:12<00:27,  5.56s/it] 78%|███████▊  | 14/18 [01:19<00:23,  5.83s/it] 83%|████████▎ | 15/18 [01:24<00:17,  5.75s/it] 89%|████████▉ | 16/18 [01:30<00:11,  5.68s/it] 94%|█████████▍| 17/18 [01:35<00:05,  5.64s/it]100%|██████████| 18/18 [01:40<00:00,  5.39s/it]100%|██████████| 18/18 [01:40<00:00,  5.58s/it]
