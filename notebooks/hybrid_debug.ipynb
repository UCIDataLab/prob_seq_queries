{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57525836-9b1d-4d86-ae13-258a3da696ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "sys.path.insert(1,\"/home/showalte/research/prob_seq_queries/\")\n",
    "from seq_queries.utils import read_pkl, write_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c30f518-8c7a-4440-aa8b-e254d198fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_of_tuples_match(tup1, tup2):\n",
    "    assert len(tup1) == len(tup2),\"Layer lengths do not match\"\n",
    "    for t1,t2 in zip(tup1, tup2):\n",
    "        assert len(t1) == len(t2),f\"Tuples of 2 do not match, t1 {t1.shape} | t2 {t2.shape}\"\n",
    "        t11,t12 = t1\n",
    "        t21,t22 = t2\n",
    "        assert torch.equal(t11,t21),\\\n",
    "            f\"First tuple position shapes do not match: t11 {t11.shape} | t21 {t21.shape}\"\n",
    "        assert torch.equal(t12,t22),\\\n",
    "            f\"Second tuple position shapes do not match: t12 {t12.shape} | t21 {t22.shape}\"\n",
    "    print(\"All good!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f63686c1-90ca-47bd-873a-baf7a5be52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_2xtups(hidden_states,max_batch_size = 16):\n",
    "    hidden_states = list(zip(*[\n",
    "                    zip(*(torch.split(h[0],max_batch_size), torch.split(h[1],max_batch_size)))\n",
    "                     for h in hidden_states]))\n",
    "    print(len(hidden_states))\n",
    "    print(len(hidden_states[0]))\n",
    "    print(len(hidden_states[0][0]))\n",
    "    print(hidden_states[0][0][0].shape)\n",
    "    return hidden_states\n",
    "\n",
    "def unsplit_2xtups(step_outputs):\n",
    "    layer_hiddens = []\n",
    "    for layer_data in zip(*step_outputs):\n",
    "            layer_data= list(zip(*layer_data))\n",
    "            layer_hiddens.append(\n",
    "                (torch.cat(layer_data[0],dim=0).cpu(),\n",
    "                 torch.cat(layer_data[1],dim=0).cpu())\n",
    "            )\n",
    "    return tuple(layer_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63fd26f2-5504-47b8-8e3a-1b22f328ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_hidden_state(num_layers, tup_size, tensor_dim):\n",
    "    tup = []\n",
    "    for l in range(num_layers):\n",
    "        i_tup = []\n",
    "        for t in range(tup_size):\n",
    "            i_tup.append(torch.randn(*tensor_dim))\n",
    "        i_tup = tuple(i_tup)\n",
    "        tup.append(i_tup)\n",
    "    return tuple(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8f4eafa-81a6-448f-951c-ed5a4718807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8\n",
    "# Samples x heads x seq_len x head_dim)\n",
    "tensor_dim = (400, 12, 10, 68)\n",
    "tup_size = 2\n",
    "h = make_hidden_state(num_layers, tup_size, tensor_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "568aacbc-49cd-4b42-b394-af16b82184f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "8\n",
      "2\n",
      "torch.Size([1, 12, 10, 68])\n",
      "All good!\n",
      "50\n",
      "8\n",
      "2\n",
      "torch.Size([8, 12, 10, 68])\n",
      "All good!\n",
      "45\n",
      "8\n",
      "2\n",
      "torch.Size([9, 12, 10, 68])\n",
      "All good!\n",
      "13\n",
      "8\n",
      "2\n",
      "torch.Size([31, 12, 10, 68])\n",
      "All good!\n",
      "25\n",
      "8\n",
      "2\n",
      "torch.Size([16, 12, 10, 68])\n",
      "All good!\n",
      "4\n",
      "8\n",
      "2\n",
      "torch.Size([128, 12, 10, 68])\n",
      "All good!\n",
      "13\n",
      "8\n",
      "2\n",
      "torch.Size([32, 12, 10, 68])\n",
      "All good!\n"
     ]
    }
   ],
   "source": [
    "max_batch_sizes = [1,8,9,31,16,128,32]\n",
    "for max_batch_size in max_batch_sizes:\n",
    "    tuple_of_tuples_match(\n",
    "        h,\n",
    "        unsplit_2xtups(\n",
    "            split_2xtups(h,\n",
    "                         max_batch_size), \n",
    "        )\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b9cf3-a637-4dbd-ad96-94c4b2ca27a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
