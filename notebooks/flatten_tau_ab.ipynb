{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54523613-671a-4c18-9bc1-8764ef06f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "sys.path.insert(1,\"/home/showalte/research/prob_seq_queries/\")\n",
    "from seq_queries.utils import read_pkl, write_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b09a07-ce42-44ff-a454-e16e6b810da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tau_ab_data(experiment,dataset,h,s,num_mc_samples, max_num_queries,\n",
    "                   methods = ['beam_search_lower_bound','importance_sampling']):\n",
    "    data_dict = {}\n",
    "    for method in methods:\n",
    "        root = f\"../data/query_4/{dataset}/{experiment}/\"\n",
    "        template_file=(f\"{experiment.replace('_','-')}_{dataset.replace('_','-')}_\"\n",
    "                + f\"query-4_{f'{h}h' if method != 'entropy_ablation' else '*'}_{s}s*\"\n",
    "                + f\"{f'_{num_mc_samples}mc' if max_num_queries else '_'}\"\n",
    "                + f\"{'_lb' if method == 'beam_search_lower_bound' else '_imp'}\"\n",
    "                + f\"{f'_{max_num_queries}q' if max_num_queries else ''}*.pkl\") \n",
    "        template_path = os.path.join(root,template_file)\n",
    "        print(template_path)\n",
    "        paths = glob.glob(template_path)\n",
    "        assert len(paths) == 1,f\"Too many or too few paths: {paths}\"\n",
    "        data_dict[method] = read_pkl(paths[0])\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d8c9ca0f-40e5-4edd-9228-21b2860a746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tau_ab_data(data_dict, method,dataset, h,s,max_num_queries,num_mc_samples):\n",
    "    df_list = []\n",
    "    for method,data in data_dict.items():\n",
    "        if method == \"beam_search_lower_bound\":\n",
    "            intermediate_query_probs = data_dict[method]['intermediate_lbs']\n",
    "        elif method == \"importance_sampling\":\n",
    "            intermediate_query_probs = data_dict[method]['intermediate_query_probs']\n",
    "        intermediate_query_probs = torch.cumsum(\n",
    "            intermediate_query_probs,\n",
    "            dim=1)\n",
    "        # print(intermediate_query_probs.shape)\n",
    "        tau_a_raw = torch.gather(\n",
    "            intermediate_query_probs,-1,\n",
    "            torch.LongTensor([[data_dict[method]['metadata']['tau_a_excl_terms']]\\\n",
    "                         *intermediate_query_probs.shape[1]]\n",
    "                         *intermediate_query_probs.shape[0]\n",
    "                        )).sum(dim=-1)\n",
    "        tau_b_raw = torch.gather(\n",
    "            intermediate_query_probs,-1,\n",
    "            torch.LongTensor([[data_dict[method]['metadata']['tau_b_excl_terms']]\\\n",
    "                         *intermediate_query_probs.shape[1]]\n",
    "                         *intermediate_query_probs.shape[0]\n",
    "                        )).sum(dim=-1)\n",
    "        # print(tau_b_raw.max(), tau_a_raw.max())\n",
    "        tau_a = pd.DataFrame(tau_a_raw)\n",
    "        tau_a['sequence_id'] = range(tau_a.shape[0])\n",
    "        tau_b = pd.DataFrame(tau_b_raw)\n",
    "\n",
    "        df_a = pd.melt(tau_a,id_vars=['sequence_id'],\n",
    "                       value_vars=[c for c in tau_a.columns if c != 'sequence_id'])\n",
    "        # print(df_a.head())\n",
    "        df_a.columns = ['sequence_id','k',f'{method}_tau_a']\n",
    "        df_b = pd.melt(tau_b,\n",
    "                       value_vars=tau_b.columns)\n",
    "        df_b.columns = ['k',f'tau_b']\n",
    "        df_a[f'{method}_tau_b'] = df_b['tau_b']\n",
    "        df_a['k'] += 1\n",
    "        # print(df_a.sequence_id.max())\n",
    "        df_list.append(df_a)\n",
    "    \n",
    "    for df in df_list[1:]:\n",
    "        df.drop('k',inplace = True,axis=1)\n",
    "    \n",
    "    final_df = pd.concat(df_list,axis=1)\n",
    "    # print(final_df.columns)\n",
    "    \n",
    "    return final_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27a3ae2f-4602-4ba3-b38d-2effcfc82412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_experiments(experiments, datasets, lengths,model_budget=False,\n",
    "                       max_num_queries=None,\n",
    "                       num_mc_samples=None):\n",
    "    data_list = []\n",
    "    for experiment in experiments:\n",
    "        for dataset in datasets:\n",
    "            for h,s in lengths:\n",
    "                data = get_tau_ab_data(experiment,dataset,\n",
    "                                           h,s,max_num_queries=max_num_queries,\n",
    "                                          num_mc_samples=num_mc_samples)\n",
    "                df = flatten_tau_ab_data(data,experiment, dataset, \n",
    "                                        h,s,max_num_queries=max_num_queries,\n",
    "                                       num_mc_samples =num_mc_samples)\n",
    "                    \n",
    "                data_list.append(df)\n",
    "                    \n",
    "    # print(len(data_list))\n",
    "    data_df = pd.concat(data_list,axis = 0)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1d551b6a-a6c8-40ba-b698-49c8705709f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare\n",
      "../data/query_4/shakespeare/val_dl/val-dl_shakespeare_query-4_10h_20s*_10000mc_lb_1000q*.pkl\n",
      "../data/query_4/shakespeare/val_dl/val-dl_shakespeare_query-4_10h_20s*_10000mc_imp_1000q*.pkl\n",
      "(31000, 7)\n",
      "No missing values: True\n",
      "=========================\n",
      "apps\n",
      "../data/query_4/apps/val_dl/val-dl_apps_query-4_10h_15s*_10000mc_lb_1000q*.pkl\n",
      "../data/query_4/apps/val_dl/val-dl_apps_query-4_10h_15s*_10000mc_imp_1000q*.pkl\n",
      "(31000, 7)\n",
      "No missing values: True\n",
      "=========================\n",
      "moocs\n",
      "../data/query_4/moocs/val_dl/val-dl_moocs_query-4_10h_15s*_10000mc_lb_1000q*.pkl\n",
      "../data/query_4/moocs/val_dl/val-dl_moocs_query-4_10h_15s*_10000mc_imp_1000q*.pkl\n",
      "(31000, 7)\n",
      "No missing values: True\n",
      "=========================\n",
      "amazon\n",
      "../data/query_4/amazon/val_dl/val-dl_amazon_query-4_10h_15s*_10000mc_lb_1000q*.pkl\n",
      "../data/query_4/amazon/val_dl/val-dl_amazon_query-4_10h_15s*_10000mc_imp_1000q*.pkl\n",
      "(31000, 7)\n",
      "No missing values: True\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "max_num_queries = 1000\n",
    "num_mc_samples = 10000\n",
    "lens = [(10,20),(10,15),(10,15),(10,15)]\n",
    "datasets = ['shakespeare','apps','moocs','amazon']\n",
    "\n",
    "for length,dataset in zip(lens,datasets):\n",
    "    print(dataset)\n",
    "    df = flatten_experiments(['val_dl'],[dataset],[length],\n",
    "                        max_num_queries=max_num_queries,\n",
    "                        num_mc_samples=num_mc_samples)\n",
    "    # print(df.beam_search_lower_bound_tau_b.describe())\n",
    "    # print(df.importance_sampling_tau_b.describe())\n",
    "    print(df.shape)\n",
    "    # print(df.head())\n",
    "    df.to_csv(f'{dataset}_query4-ablation_30k.csv',index=None)\n",
    "    print(f\"No missing values: {(df.isnull().sum()==0).all()}\")\n",
    "    print(\"=====\"*5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06072dd-a261-49cd-9e50-2997857618ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e5d79-d382-43de-9b6c-3023c76da67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
