{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5fc0db-8671-45eb-9006-8236cda57d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "sys.path.insert(1,\"/home/showalte/research/prob_seq_queries/\")\n",
    "from seq_queries.utils import read_pkl, write_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b52721a-1c0f-4001-b451-3899e8e6176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_data(experiment, dataset, h, s, root=\"../data\", \n",
    "            methods=['beam_search_is_hybrid','importance_sampling'],\n",
    "            gt_methods=['ground_truth','beam_search']):\n",
    "    \n",
    "        data_dict = {}\n",
    "        gt_type=None\n",
    "        for method in methods:\n",
    "            template_path=root + f\"/{method}/{dataset}/{experiment}/\"\n",
    "            template_file=f\"{experiment.replace('_','-')}_{dataset.replace('_','-')}_{method.replace('_','-')}_{h}h_{s}s*.pkl\"\n",
    "            pot_pattern = os.path.join(template_path,template_file)\n",
    "            pot_paths = glob.glob(pot_pattern)\n",
    "            assert len(pot_paths) == 1,\\\n",
    "                f\"Found {len(pot_paths)} paths for {pot_pattern}\"\n",
    "            data_dict[method]= read_pkl(pot_paths[0])\n",
    "            data_dict[method]['metadata']['result_filepath'] = pot_paths[0]\n",
    "        for gt_method in gt_methods:\n",
    "            try:\n",
    "                template_path=root + f\"/{gt_method}/{dataset}/{experiment}/\"\n",
    "                template_file=f\"{experiment.replace('_','-')}_{dataset.replace('_','-')}_{gt_method.replace('_','-')}_{h}h_{s}s*.pkl\"\n",
    "                pot_pattern = os.path.join(template_path,template_file)\n",
    "                pot_paths = glob.glob(pot_pattern)\n",
    "                assert len(pot_paths) == 1,\\\n",
    "                    f\"Found {len(pot_paths)} paths for {pot_pattern}\"\n",
    "                # print(pot_paths[0])\n",
    "                data_dict[gt_method]= read_pkl(pot_paths[0])\n",
    "                data_dict[gt_method]['metadata']['result_filepath'] = pot_paths[0]\n",
    "                data_dict['gt_type'] = gt_method\n",
    "                return data_dict\n",
    "            except: pass\n",
    "        assert False,\"Could not find ground truth\"\n",
    "        return None\n",
    "                \n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87bf22-bcf6-49ea-b43c-9c6bd34216f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Estimate fields\n",
    "- ground_truth/beam_search_lb\n",
    "- Importance sampling estimate\n",
    "- Hybrid estimate\n",
    "- Importance sampling variance\n",
    "- Hybrid variance\n",
    "\n",
    "# General Metadata\n",
    "- dataset_name\n",
    "- experiment_name\n",
    "- history_id\n",
    "- Excluded term\n",
    "- sequence_length\n",
    "- history_length\n",
    "- total_sequence_length\n",
    "\n",
    "# Sampling metadata\n",
    "- num_mc_samples (sub_estimates)\n",
    "- sample_model_iters\n",
    "\n",
    "# Hybrid data\n",
    "- hybrid_model_iters\n",
    "\n",
    "# Beam search metadata\n",
    "- min_variance\n",
    "- search_model_iters\n",
    "- min_variance_reduction\n",
    "- true_coverage\n",
    "- restricted_coverage\n",
    "- num_beams\n",
    "- top_k\n",
    "- top_p\n",
    "- (beam search) interpolation_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bff8c61a-43ae-4901-8cf0-1f34078701f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flatten_hybrid_sampling(samp_dict,sub_estimates,sample_type=\"importance\"):\n",
    "#     samp_estimates = samp_dict['hybrid_bs_is_estimate'][:,:len(sub_estimates)]\n",
    "#     if not sub_estimates:\n",
    "#         if len(samp_estimates.shape) ==3:\n",
    "#             assert samp_estimates.shape[1] == samp_dict['metadata']['num_mc_samples'],\\\n",
    "#             (f\"Error, estimate dimensions are {samp_estimates.shape} but the number of samples is\" +\n",
    "#              f\"{samp_dict['metadata']['num_mc_samples']}, which does not match\")\n",
    "            \n",
    "#             samp_estimates = torch.gather(samp_estimates.mean(dim=1),1,\n",
    "#                                           samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "#             assert len(samp_estimates.shape) == 1,f\"Shape of imp_samp_estimates is {len(samp_estimates.shape)}\"\n",
    "#         if len(samp_estimates.shape) ==2:\n",
    "#             samp_estimates = torch.gather(samp_estimates,1,\n",
    "#                                               samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "        \n",
    "#         df = pd.DataFrame(samp_estimates)\n",
    "#         df.insert(0,'num_mc_samples',samp_dict['metadata']['num_mc_samples'])\n",
    "#         df[f'{sample_type}_model_iters'] = samp_dict['model_iters']\n",
    "#         df[f'{sample_type}_est_variance'] = samp_dist['hyrbid_var']\n",
    "        \n",
    "#     else:\n",
    "#         assert samp_estimates.shape[1] == len(sub_estimates),\\\n",
    "#         (\"Importance sampling estimates and sub_estimates are not aligned in shape.\" +\n",
    "#          f\"got sample_est: {samp_estimates.shape[1]} and sub_estimates: {len(sub_estimates)}\")\n",
    "#         print(samp_estimates.shape)\n",
    "#         if len(samp_estimates.shape) == 2:\n",
    "#             samp_estimates = pd.DataFrame(samp_estimates,columns=sub_estimates)\n",
    "#             samp_var = pd.DataFrame(samp_dict['hybrid_var'][:,:len(sub_estimates)],columns=sub_estimates)\n",
    "#             model_iters_df = pd.DataFrame(samp_dict['model_iters'][:,:len(sub_estimates)],columns=sub_estimates)\n",
    "#             df = pd.melt(samp_estimates,value_vars=sub_estimates)\n",
    "#             df.columns = ['num_samples','sample_estimate']\n",
    "#             print(df.head())\n",
    "#             var_df =  pd.melt(samp_var,value_vars=sub_estimates)\n",
    "#             var_df.columns = ['num_samples','variance']\n",
    "#             iter_df = pd.melt(model_iters_df,value_vars=sub_estimates)\n",
    "#             iter_df.columns = ['num_samples','model_iters']\n",
    "#             df[f'{sample_type}_model_iters']=iter_df['model_iters']\n",
    "#             df[f'{sample_type}_est_variance']=var_df['variance']\n",
    "#             print(\"length 2\")\n",
    "#             print(df.shape)\n",
    "            \n",
    "#         elif len(samp_estimates.shape) == 3:\n",
    "#             df_list = []\n",
    "#             for i in range(len(sub_estimates)):\n",
    "#                 df = pd.DataFrame(\n",
    "#                             torch.gather(samp_estimates[:,i],1,\n",
    "#                                       samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "#                         )\n",
    "#                 df.insert(0,'num_mc_samples',sub_estimates[i])\n",
    "#                 df[f'{sample_type}_model_iters'] = samp_dict['model_iters'][:,i]\n",
    "#                 df[f'{sample_type}_est_variance'] = samp_dict['sample_est_var'][:,i]\n",
    "#                 print(df.shape)\n",
    "#                 df_list.append(df)\n",
    "            \n",
    "#             df = pd.concat(df_list,axis=0,ignore_index=True)\n",
    "#         else:\n",
    "#             assert False,f\"Shape of samp_estimates is {len(samp_estimates.shape)}\"\n",
    "#     assert df.shape[-1] == 4, f\"DF shape is {df.shape}\"\n",
    "#     df.columns = ['num_mc_samples',f'{sample_type}_sampling_est',\n",
    "#                   f'{sample_type}_model_iters',f'{sample_type}_est_variance']\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "def flatten_sampling(samp_dict,sub_estimates,sample_type=\"importance\"):\n",
    "    samp_estimates = samp_dict['sample_estimates'][:,:len(sub_estimates)]\n",
    "    if not sub_estimates:\n",
    "        if len(samp_estimates.shape) ==3:\n",
    "            assert samp_estimates.shape[1] == samp_dict['metadata']['num_mc_samples'],\\\n",
    "            (f\"Error, estimate dimensions are {samp_estimates.shape} but the number of samples is\" +\n",
    "             f\"{samp_dict['metadata']['num_mc_samples']}, which does not match\")\n",
    "            \n",
    "            samp_estimates = torch.gather(samp_estimates.mean(dim=1),1,\n",
    "                                          samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "            assert len(samp_estimates.shape) == 1,f\"Shape of imp_samp_estimates is {len(samp_estimates.shape)}\"\n",
    "        if len(samp_estimates.shape) ==2:\n",
    "            samp_estimates = torch.gather(samp_estimates,1,\n",
    "                                              samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "        \n",
    "        df = pd.DataFrame(samp_estimates)\n",
    "        df.insert(0,'num_mc_samples',samp_dict['metadata']['num_mc_samples'])\n",
    "        df[f'{sample_type}_model_iters'] = samp_dict['model_iters']\n",
    "        df[f'{sample_type}_est_variance'] = samp_dist['sample_estimate_var']\n",
    "        \n",
    "    else:\n",
    "        assert samp_estimates.shape[1] == len(sub_estimates),\\\n",
    "        (\"Importance sampling estimates and sub_estimates are not aligned in shape.\" +\n",
    "         f\"got sample_est: {samp_estimates.shape[1]} and sub_estimates: {len(sub_estimates)}\")\n",
    "        if len(samp_estimates.shape) == 2:\n",
    "            samp_estimates = pd.DataFrame(samp_estimates,columns=sub_estimates)\n",
    "            samp_var = pd.DataFrame(samp_dict['sample_estimate_var'][:,:len(sub_estimates)],columns=sub_estimates)\n",
    "            model_iters_df = pd.DataFrame(samp_dict['model_iters'][:,:len(sub_estimates)],columns=sub_estimates)\n",
    "            df = pd.melt(samp_estimates,value_vars=sub_estimates)\n",
    "            df.columns = ['num_samples','sample_estimate']\n",
    "            var_df =  pd.melt(samp_var,value_vars=sub_estimates)\n",
    "            var_df.columns = ['num_samples','variance']\n",
    "            iter_df = pd.melt(model_iters_df,value_vars=sub_estimates)\n",
    "            iter_df.columns = ['num_samples','model_iters']\n",
    "            df[f'{sample_type}_model_iters']=iter_df['model_iters']\n",
    "            df[f'{sample_type}_est_variance']=var_df['variance']\n",
    "            \n",
    "        elif len(samp_estimates.shape) == 3:\n",
    "            df_list = []\n",
    "            for i in range(len(sub_estimates)):\n",
    "                df = pd.DataFrame(\n",
    "                            torch.gather(samp_estimates[:,i],1,\n",
    "                                      samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "                        )\n",
    "                df.insert(0,'num_mc_samples',samp_dict['metadata']['num_mc_samples'])\n",
    "                df[f'{sample_type}_model_iters'] = samp_dict['model_iters'][:,i]\n",
    "                samp_est_var = torch.gather(samp_dict['sample_estimate_var'][:,i],1,\n",
    "                                            samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "                                            \n",
    "                df[f'{sample_type}_est_variance'] = samp_est_var\n",
    "                df_list.append(df)\n",
    "            df = pd.concat(df_list,axis=0,ignore_index=True)\n",
    "        else:\n",
    "            assert False,f\"Shape of samp_estimates is {len(samp_estimates.shape)}\"\n",
    "    assert df.shape[-1] == 4, f\"DF shape is {df.shape}\"\n",
    "    df.columns = ['num_mc_samples',f'{sample_type}_sampling_est',\n",
    "                  f'{sample_type}_model_iters',f'{sample_type}_est_variance']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def flatten_gt(data_dict,gt_type):\n",
    "    gt_dict = data_dict[gt_type]\n",
    "    # gt = gt_dict['bs_lower_bound']\n",
    "    gt = torch.gather(gt_dict['dist_lower_bound'],1,\n",
    "                      gt_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "    assert len(gt.shape) == 1,\\\n",
    "    f\"Ground truth has {len(gt.shape)} dimensions\"\n",
    "    df = pd.DataFrame(gt,columns=['ground_truth'])\n",
    "    for item in ['true_coverage','restricted_coverage']:\n",
    "        df[item] = [gti.item() for gti in gt_dict[item]]\n",
    "    # df[\"gt_model_iters\"] = gt_dict['model_iters']\n",
    "    df['gt_type'] = gt_type\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def flatten_experiment(data_dict,experiment, dataset,h,s,\n",
    "     global_agreement_vals= ['excluded_terms']):\n",
    "    sub_estimates = sorted(list(\n",
    "        set(data_dict['importance_sampling']['metadata']['sub_estimates']) &\n",
    "        set(data_dict['beam_search_is_hybrid']['metadata']['sub_estimates'])))\n",
    "    sub_est_len = 1 if not sub_estimates else len(sub_estimates)\n",
    "    importance_df = flatten_sampling(data_dict['importance_sampling'],sub_estimates,sample_type ='importance')\n",
    "    # print(importance_df.shape)\n",
    "    hybrid_df = flatten_sampling(data_dict['beam_search_is_hybrid'],sub_estimates,sample_type ='hybrid')\n",
    "    # print(hybrid_df.shape)\n",
    "    hybrid_df.drop(\"num_mc_samples\",inplace = True,axis=1)\n",
    "    gt_df = flatten_gt(data_dict,data_dict['gt_type'])\n",
    "    gt_df = pd.concat([gt_df]*sub_est_len,axis=0,ignore_index=True)\n",
    "    final_df = pd.concat([importance_df,hybrid_df,gt_df],axis=1)\n",
    "    # print(hybrid_df.head())\n",
    "    # print(importance_df.head())\n",
    "    # print(final_df.isnull().sum())\n",
    "    \n",
    "    # Metadata checks\n",
    "    is_metadata = ['top_k','top_p']\n",
    "    bs_metadata = ['min_variance','min_var_reduction','num_beams']\n",
    "    for m in is_metadata:\n",
    "        final_df[m] = data_dict['importance_sampling']['metadata'][m]\n",
    "    for m in bs_metadata:\n",
    "        final_df[m] = data_dict[data_dict['gt_type']]['metadata'][m]\n",
    "    final_df['interp_func'] = str(data_dict[data_dict['gt_type']]['metadata']['interp_func']).split(\" \")[1]\n",
    "    \n",
    "    \n",
    "    final_df['dataset_name'] = dataset\n",
    "    final_df['hist_len'] = h\n",
    "    final_df['total_seq_len'] = s\n",
    "    final_df['seq_len'] = s-h\n",
    "    sequence_ids = list(range(data_dict['importance_sampling']['sample_estimates'].shape[0]))*sub_est_len\n",
    "    excluded_terms = data_dict['importance_sampling']['excluded_terms'].numpy().tolist()*sub_est_len\n",
    "    final_df['sequence_id'] = sequence_ids\n",
    "    final_df['excluded_term'] = excluded_terms\n",
    "    \n",
    "    return final_df\n",
    "     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb936c89-eb0e-42cd-923b-8a60a9fc167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = [\"val_dl\"]\n",
    "dataset = [\"apps\"]\n",
    "lengths = [(13,15),(12,15)]\n",
    "def flatten_experiments(experiments, datasets, lengths):\n",
    "    data_list = []\n",
    "    for experiment in experiments:\n",
    "        for dataset in datasets:\n",
    "            for h,s in lengths:\n",
    "                # try:\n",
    "                    \n",
    "                    data = get_experiment_data(experiment,dataset,h,s)\n",
    "                    df = flatten_experiment(data,experiment, dataset, h,s)\n",
    "                    # print(df.head())\n",
    "                    # print(df.columns)\n",
    "                    # print(df.shape)\n",
    "                    # sys.exit(1)\n",
    "                    data_list.append(df)\n",
    "                # except Exception as e:\n",
    "                #     print(\"Could not flatten: Experiment: {} | Dataset: {} | lengths: {}\"\\\n",
    "                #           .format(experiment, dataset, (h,s)))\n",
    "                    \n",
    "    # print(len(data_list))\n",
    "    data_df = pd.concat(data_list,axis = 0)\n",
    "    ordering = [ 'dataset_name','sequence_id','seq_len', 'excluded_term', 'gt_type','ground_truth','importance_sampling_est','hybrid_sampling_est', \n",
    "                'num_mc_samples', 'importance_model_iters','hybrid_model_iters',\n",
    "       'importance_est_variance',  'hybrid_est_variance',\n",
    "         'true_coverage', 'restricted_coverage',  'top_k', 'top_p', 'min_variance',\n",
    "       'min_var_reduction', 'num_beams', 'interp_func',\n",
    "       'hist_len', 'total_seq_len', ]\n",
    "    data_df = data_df[ordering]\n",
    "    print(data_df.columns)\n",
    "    \n",
    "    return data_df\n",
    "                \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "62c51bbf-1517-42cc-8ed2-b99b496d041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dataset_name', 'sequence_id', 'seq_len', 'excluded_term', 'gt_type',\n",
      "       'ground_truth', 'importance_sampling_est', 'hybrid_sampling_est',\n",
      "       'num_mc_samples', 'importance_model_iters', 'hybrid_model_iters',\n",
      "       'importance_est_variance', 'hybrid_est_variance', 'true_coverage',\n",
      "       'restricted_coverage', 'top_k', 'top_p', 'min_variance',\n",
      "       'min_var_reduction', 'num_beams', 'interp_func', 'hist_len',\n",
      "       'total_seq_len'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = flatten_experiments(['val_dl'],['amazon'],[(13,15),(12,15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6ce843b1-19fd-4a08-bd5e-a76576456916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38304, 23)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data['importance_sampling']['metadata']['sub_estimates']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a3e8a10-b1b5-4498-bc4a-90eaa5ecbba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>excluded_term</th>\n",
       "      <th>gt_type</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>importance_sampling_est</th>\n",
       "      <th>hybrid_sampling_est</th>\n",
       "      <th>num_mc_samples</th>\n",
       "      <th>importance_model_iters</th>\n",
       "      <th>...</th>\n",
       "      <th>true_coverage</th>\n",
       "      <th>restricted_coverage</th>\n",
       "      <th>top_k</th>\n",
       "      <th>top_p</th>\n",
       "      <th>min_variance</th>\n",
       "      <th>min_var_reduction</th>\n",
       "      <th>num_beams</th>\n",
       "      <th>interp_func</th>\n",
       "      <th>hist_len</th>\n",
       "      <th>total_seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>ground_truth</td>\n",
       "      <td>0.031582</td>\n",
       "      <td>0.043228</td>\n",
       "      <td>0.065068</td>\n",
       "      <td>10000</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lin_interp</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>ground_truth</td>\n",
       "      <td>0.036299</td>\n",
       "      <td>0.046776</td>\n",
       "      <td>0.062245</td>\n",
       "      <td>10000</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930921</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lin_interp</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazon</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>ground_truth</td>\n",
       "      <td>0.086385</td>\n",
       "      <td>0.022673</td>\n",
       "      <td>0.022827</td>\n",
       "      <td>10000</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.855197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lin_interp</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>ground_truth</td>\n",
       "      <td>0.077772</td>\n",
       "      <td>0.069268</td>\n",
       "      <td>0.069139</td>\n",
       "      <td>10000</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154921</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lin_interp</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>ground_truth</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.116185</td>\n",
       "      <td>0.109567</td>\n",
       "      <td>10000</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lin_interp</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_name  sequence_id  seq_len  excluded_term       gt_type  \\\n",
       "0       amazon            0        2             22  ground_truth   \n",
       "1       amazon            1        2             22  ground_truth   \n",
       "2       amazon            2        2             13  ground_truth   \n",
       "3       amazon            3        2             13  ground_truth   \n",
       "4       amazon            4        2             13  ground_truth   \n",
       "\n",
       "   ground_truth  importance_sampling_est  hybrid_sampling_est  num_mc_samples  \\\n",
       "0      0.031582                 0.043228             0.065068           10000   \n",
       "1      0.036299                 0.046776             0.062245           10000   \n",
       "2      0.086385                 0.022673             0.022827           10000   \n",
       "3      0.077772                 0.069268             0.069139           10000   \n",
       "4      0.002445                 0.116185             0.109567           10000   \n",
       "\n",
       "   importance_model_iters  ...  true_coverage  restricted_coverage  top_k  \\\n",
       "0                     150  ...       0.903011                  1.0      0   \n",
       "1                     150  ...       0.930921                  1.0      0   \n",
       "2                     150  ...       0.855197                  1.0      0   \n",
       "3                     150  ...       0.154921                  1.0      0   \n",
       "4                     150  ...       0.998659                  1.0      0   \n",
       "\n",
       "   top_p  min_variance  min_var_reduction  num_beams  interp_func  hist_len  \\\n",
       "0      0         False                0.9        1.0   lin_interp        13   \n",
       "1      0         False                0.9        1.0   lin_interp        13   \n",
       "2      0         False                0.9        1.0   lin_interp        13   \n",
       "3      0         False                0.9        1.0   lin_interp        13   \n",
       "4      0         False                0.9        1.0   lin_interp        13   \n",
       "\n",
       "   total_seq_len  \n",
       "0             15  \n",
       "1             15  \n",
       "2             15  \n",
       "3             15  \n",
       "4             15  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a52b7fd-4659-494d-9ff3-13584d46e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('padhraic_extract_amazon_12-13_15.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193194d0-d619-4e39-ba5f-ee253a686bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
