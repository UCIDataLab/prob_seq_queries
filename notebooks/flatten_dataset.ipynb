{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5fc0db-8671-45eb-9006-8236cda57d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "sys.path.insert(1,\"/home/showalte/research/prob_seq_queries/\")\n",
    "from seq_queries.utils import read_pkl, write_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b52721a-1c0f-4001-b451-3899e8e6176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_data(experiment, dataset, h, s, root=\"../data\", \n",
    "            methods=['beam_search_is_hybrid','importance_sampling'],\n",
    "            gt_methods=['ground_truth','beam_search']):\n",
    "    \n",
    "        data_dict = {}\n",
    "        gt_type=None\n",
    "        for method in methods:\n",
    "            template_path=root + f\"/{method}/{dataset}/{experiment}/\"\n",
    "            template_file=f\"{experiment.replace('_','-')}_{dataset.replace('_','-')}_{method.replace('_','-')}_{h}h_{s}s*.pkl\"\n",
    "            pot_pattern = os.path.join(template_path,template_file)\n",
    "            pot_paths = glob.glob(pot_pattern)\n",
    "            assert len(pot_paths) == 1,\\\n",
    "                f\"Found {len(pot_paths)} paths for {pot_pattern}\"\n",
    "            data_dict[method]= read_pkl(pot_paths[0])\n",
    "            data_dict[method]['metadata']['result_filepath'] = pot_paths[0]\n",
    "        for gt_method in gt_methods:\n",
    "            try:\n",
    "                template_path=root + f\"/{gt_method}/{dataset}/{experiment}/\"\n",
    "                template_file=f\"{experiment.replace('_','-')}_{dataset.replace('_','-')}_{gt_method.replace('_','-')}_{h}h_{s}s*.pkl\"\n",
    "                pot_pattern = os.path.join(template_path,template_file)\n",
    "                pot_paths = glob.glob(pot_pattern)\n",
    "                assert len(pot_paths) == 1,\\\n",
    "                    f\"Found {len(pot_paths)} paths for {pot_pattern}\"\n",
    "                # print(pot_paths[0])\n",
    "                data_dict[gt_method]= read_pkl(pot_paths[0])\n",
    "                data_dict[gt_method]['metadata']['result_filepath'] = pot_paths[0]\n",
    "                data_dict['gt_type'] = gt_method\n",
    "                return data_dict\n",
    "            except: pass\n",
    "        assert False,\"Could not find ground truth\"\n",
    "        return None\n",
    "                \n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87bf22-bcf6-49ea-b43c-9c6bd34216f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Estimate fields\n",
    "- ground_truth/beam_search_lb\n",
    "- Importance sampling estimate\n",
    "- Hybrid estimate\n",
    "- Importance sampling variance\n",
    "- Hybrid variance\n",
    "\n",
    "# General Metadata\n",
    "- dataset_name\n",
    "- experiment_name\n",
    "- history_id\n",
    "- Excluded term\n",
    "- sequence_length\n",
    "- history_length\n",
    "- total_sequence_length\n",
    "\n",
    "# Sampling metadata\n",
    "- num_mc_samples (sub_estimates)\n",
    "- sample_model_iters\n",
    "\n",
    "# Hybrid data\n",
    "- hybrid_model_iters\n",
    "\n",
    "# Beam search metadata\n",
    "- min_variance\n",
    "- search_model_iters\n",
    "- min_variance_reduction\n",
    "- true_coverage\n",
    "- restricted_coverage\n",
    "- num_beams\n",
    "- top_k\n",
    "- top_p\n",
    "- (beam search) interpolation_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bff8c61a-43ae-4901-8cf0-1f34078701f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_sampling(samp_dict,sub_estimates,sample_type=\"importance\"):\n",
    "    samp_estimates = samp_dict['sample_estimates'][:,:len(sub_estimates)]\n",
    "    if not sub_estimates:\n",
    "        if len(samp_estimates.shape) ==3:\n",
    "            assert samp_estimates.shape[1] == samp_dict['metadata']['num_mc_samples'],\\\n",
    "            (f\"Error, estimate dimensions are {samp_estimates.shape} but the number of samples is\" +\n",
    "             f\"{samp_dict['metadata']['num_mc_samples']}, which does not match\")\n",
    "            \n",
    "            samp_estimates = torch.gather(samp_estimates.mean(dim=1),1,\n",
    "                                          samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "            assert len(samp_estimates.shape) == 1,f\"Shape of imp_samp_estimates is {len(samp_estimates.shape)}\"\n",
    "        if len(samp_estimates.shape) ==2:\n",
    "            samp_estimates = torch.gather(samp_estimates,1,\n",
    "                                              samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "        \n",
    "        df = pd.DataFrame(samp_estimates)\n",
    "        df.insert(0,'num_mc_samples',samp_dict['metadata']['num_mc_samples'])\n",
    "        df[f'{sample_type}_model_iters'] = samp_dict['model_iters']\n",
    "        df[f'{sample_type}_est_variance'] = samp_dist['sample_est_var']\n",
    "        \n",
    "    else:\n",
    "        assert samp_estimates.shape[1] == len(sub_estimates),\\\n",
    "        (\"Importance sampling estimates and sub_estimates are not aligned in shape.\" +\n",
    "         f\"got sample_est: {samp_estimates.shape[1]} and sub_estimates: {len(sub_estimates)}\")\n",
    "        if len(samp_estimates.shape) == 2:\n",
    "            samp_estimates = pd.DataFrame(samp_estimates,columns=sub_estimates)\n",
    "            samp_var = pd.DataFrame(samp_dict['sample_est_var'][:,:len(sub_estimates)],columns=sub_estimates)\n",
    "            model_iters_df = pd.DataFrame(samp_dict['model_iters'][:,:len(sub_estimates)],columns=sub_estimates)\n",
    "            df = pd.melt(samp_estimates,value_vars=sub_estimates)\n",
    "            df.columns = ['num_samples','sample_estimate']\n",
    "            var_df =  pd.melt(samp_var,value_vars=sub_estimates)\n",
    "            var_df.columns = ['num_samples','variance']\n",
    "            iter_df = pd.melt(model_iters_df,value_vars=sub_estimates)\n",
    "            iter_df.columns = ['num_samples','model_iters']\n",
    "            df[f'{sample_type}_model_iters']=iter_df['model_iters']\n",
    "            df[f'{sample_type}_est_variance']=var_df['variance']\n",
    "            \n",
    "        elif len(samp_estimates.shape) == 3:\n",
    "            df_list = []\n",
    "            for i in range(len(sub_estimates)):\n",
    "                df = pd.DataFrame(\n",
    "                            torch.gather(samp_estimates[:,i],1,\n",
    "                                      samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "                        )\n",
    "                df[f'{sample_type}_model_iters'] = samp_dict['model_iters'][:,i]\n",
    "                df['num_mc_samples'] = sub_estimates[i]\n",
    "            df = pd.concat(df_list,axis=0,ignore_index=True)\n",
    "        else:\n",
    "            assert False,f\"Shape of samp_estimates is {len(samp_estimates.shape)}\"\n",
    "    assert df.shape[-1] == 4, f\"DF shape is {df.shape}\"\n",
    "    df.columns = ['num_mc_samples',f'{sample_type}_sampling_est',\n",
    "                  f'{sample_type}_model_iters',f'{sample_type}_est_variance']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def flatten_hybrid_sampling(samp_dict,sub_estimates,sample_type=\"importance\"):\n",
    "    samp_estimates = samp_dict['hybrid_bs_is_estimate'][:,:len(sub_estimates)]\n",
    "    if not sub_estimates:\n",
    "        if len(samp_estimates.shape) ==3:\n",
    "            assert samp_estimates.shape[1] == samp_dict['metadata']['num_mc_samples'],\\\n",
    "            (f\"Error, estimate dimensions are {samp_estimates.shape} but the number of samples is\" +\n",
    "             f\"{samp_dict['metadata']['num_mc_samples']}, which does not match\")\n",
    "            \n",
    "            samp_estimates = torch.gather(samp_estimates.mean(dim=1),1,\n",
    "                                          samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "            assert len(samp_estimates.shape) == 1,f\"Shape of imp_samp_estimates is {len(samp_estimates.shape)}\"\n",
    "        if len(samp_estimates.shape) ==2:\n",
    "            samp_estimates = torch.gather(samp_estimates,1,\n",
    "                                              samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "        \n",
    "        df = pd.DataFrame(samp_estimates)\n",
    "        df.insert(0,'num_mc_samples',samp_dict['metadata']['num_mc_samples'])\n",
    "        df[f'{sample_type}_model_iters'] = samp_dict['model_iters']\n",
    "        df[f'{sample_type}_est_variance'] = samp_dist['hyrbid_var']\n",
    "        \n",
    "    else:\n",
    "        assert samp_estimates.shape[1] == len(sub_estimates),\\\n",
    "        (\"Importance sampling estimates and sub_estimates are not aligned in shape.\" +\n",
    "         f\"got sample_est: {samp_estimates.shape[1]} and sub_estimates: {len(sub_estimates)}\")\n",
    "        if len(samp_estimates.shape) == 2:\n",
    "            samp_estimates = pd.DataFrame(samp_estimates,columns=sub_estimates)\n",
    "            samp_var = pd.DataFrame(samp_dict['hybrid_var'][:,:len(sub_estimates)],columns=sub_estimates)\n",
    "            model_iters_df = pd.DataFrame(samp_dict['model_iters'][:,:len(sub_estimates)],columns=sub_estimates)\n",
    "            df = pd.melt(samp_estimates,value_vars=sub_estimates)\n",
    "            df.columns = ['num_samples','sample_estimate']\n",
    "            print(df.head())\n",
    "            var_df =  pd.melt(samp_var,value_vars=sub_estimates)\n",
    "            var_df.columns = ['num_samples','variance']\n",
    "            iter_df = pd.melt(model_iters_df,value_vars=sub_estimates)\n",
    "            iter_df.columns = ['num_samples','model_iters']\n",
    "            df[f'{sample_type}_model_iters']=iter_df['model_iters']\n",
    "            df[f'{sample_type}_est_variance']=var_df['variance']\n",
    "            \n",
    "        elif len(samp_estimates.shape) == 3:\n",
    "            df_list = []\n",
    "            for i in range(len(sub_estimates)):\n",
    "                df = pd.DataFrame(\n",
    "                            torch.gather(samp_estimates[:,i],1,\n",
    "                                      samp_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "                        )\n",
    "                df[f'{sample_type}_model_iters'] = samp_dict['model_iters'][:,i]\n",
    "                df['num_mc_samples'] = sub_estimates[i]\n",
    "            df = pd.concat(df_list,axis=0,ignore_index=True)\n",
    "        else:\n",
    "            assert False,f\"Shape of samp_estimates is {len(samp_estimates.shape)}\"\n",
    "    assert df.shape[-1] == 4, f\"DF shape is {df.shape}\"\n",
    "    df.columns = ['num_mc_samples',f'{sample_type}_sampling_est',\n",
    "                  f'{sample_type}_model_iters',f'{sample_type}_est_variance']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def flatten_gt(data_dict,gt_type):\n",
    "    gt_dict = data_dict[gt_type]\n",
    "    # gt = gt_dict['bs_lower_bound']\n",
    "    gt = torch.gather(gt_dict['dist_lower_bound'],1,\n",
    "                      gt_dict['excluded_terms'].unsqueeze(-1)).squeeze()\n",
    "    assert len(gt.shape) == 1,\\\n",
    "    f\"Ground truth has {len(gt.shape)} dimensions\"\n",
    "    df = pd.DataFrame(gt,columns=['ground_truth'])\n",
    "    for item in ['true_coverage','restricted_coverage']:\n",
    "        df[item] = [gti.item() for gti in gt_dict[item]]\n",
    "    # df[\"gt_model_iters\"] = gt_dict['model_iters']\n",
    "    df['gt_type'] = gt_type\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def flatten_experiment(data_dict,experiment, dataset,h,s,\n",
    "     global_agreement_vals= ['excluded_terms']):\n",
    "    sub_estimates = sorted(list(\n",
    "        set(data_dict['importance_sampling']['metadata']['sub_estimates']) &\n",
    "        set(data_dict['beam_search_is_hybrid']['metadata']['sub_estimates'])))\n",
    "    sub_est_len = 1 if not sub_estimates else len(sub_estimates)\n",
    "    importance_df = flatten_sampling(data_dict['importance_sampling'],sub_estimates,sample_type ='importance')\n",
    "    # print(importance_df.shape)\n",
    "    hybrid_df = flatten_hybrid_sampling(data_dict['beam_search_is_hybrid'],sub_estimates,sample_type ='hybrid')\n",
    "    # print(hybrid_df.shape)\n",
    "    hybrid_df.drop(\"num_mc_samples\",inplace = True,axis=1)\n",
    "    gt_df = flatten_gt(data_dict,data_dict['gt_type'])\n",
    "    gt_df = pd.concat([gt_df]*sub_est_len,axis=0,ignore_index=True)\n",
    "    final_df = pd.concat([importance_df,hybrid_df,gt_df],axis=1)\n",
    "    # print(hybrid_df.head())\n",
    "    # print(importance_df.head())\n",
    "    # print(final_df.isnull().sum())\n",
    "    \n",
    "    # Metadata checks\n",
    "    is_metadata = ['top_k','top_p']\n",
    "    bs_metadata = ['min_variance','min_var_reduction','num_beams']\n",
    "    for m in is_metadata:\n",
    "        final_df[m] = data_dict['importance_sampling']['metadata'][m]\n",
    "    for m in bs_metadata:\n",
    "        final_df[m] = data_dict[data_dict['gt_type']]['metadata'][m]\n",
    "    final_df['interp_func'] = str(data_dict[data_dict['gt_type']]['metadata']['interp_func']).split(\" \")[1]\n",
    "    \n",
    "    \n",
    "    final_df['dataset_name'] = dataset\n",
    "    final_df['hist_len'] = h\n",
    "    final_df['total_seq_len'] = s\n",
    "    final_df['seq_len'] = s-h\n",
    "    sequence_ids = list(range(data_dict['importance_sampling']['sample_estimates'].shape[0]))*sub_est_len\n",
    "    excluded_terms = data_dict['importance_sampling']['excluded_terms'].numpy().tolist()*sub_est_len\n",
    "    final_df['sequence_id'] = sequence_ids\n",
    "    final_df['excluded_term'] = excluded_terms\n",
    "    \n",
    "    return final_df\n",
    "     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb936c89-eb0e-42cd-923b-8a60a9fc167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = [\"val_dl\"]\n",
    "dataset = [\"shakespeare\"]\n",
    "lengths = [(18,20), (17,20)]\n",
    "def flatten_experiments(experiments, datasets, lengths):\n",
    "    data_list = []\n",
    "    for experiment in experiments:\n",
    "        for dataset in datasets:\n",
    "            for h,s in lengths:\n",
    "                # try:\n",
    "                    \n",
    "                    data = get_experiment_data(experiment,dataset,h,s)\n",
    "                    df = flatten_experiment(data,experiment, dataset, h,s)\n",
    "                    # print(df.head())\n",
    "                    # print(df.columns)\n",
    "                    # print(df.shape)\n",
    "                    # sys.exit(1)\n",
    "                    data_list.append(df)\n",
    "                # except Exception as e:\n",
    "                #     print(\"Could not flatten: Experiment: {} | Dataset: {} | lengths: {}\"\\\n",
    "                #           .format(experiment, dataset, (h,s)))\n",
    "                    \n",
    "    # print(len(data_list))\n",
    "    data_df = pd.concat(data_list,axis = 0)\n",
    "    ordering = [ 'dataset_name','sequence_id','seq_len', 'excluded_term', 'gt_type','ground_truth','importance_sampling_est','hybrid_sampling_est', \n",
    "                'num_mc_samples', 'importance_model_iters','hybrid_model_iters',\n",
    "       'importance_est_variance',  'hybrid_est_variance',\n",
    "         'true_coverage', 'restricted_coverage',  'top_k', 'top_p', 'min_variance',\n",
    "       'min_var_reduction', 'num_beams', 'interp_func',\n",
    "       'hist_len', 'total_seq_len', ]\n",
    "    data_df = data_df[ordering]\n",
    "    print(data_df.columns)\n",
    "    \n",
    "    return data_df\n",
    "                \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62c51bbf-1517-42cc-8ed2-b99b496d041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_samples  sample_estimate\n",
      "0           10         0.621703\n",
      "1           10         0.458255\n",
      "2           10         0.042969\n",
      "3           10         0.089890\n",
      "4           10         0.212578\n",
      "   num_samples  sample_estimate\n",
      "0           10         0.111616\n",
      "1           10         0.547014\n",
      "2           10         0.033594\n",
      "3           10         0.071077\n",
      "4           10         0.025806\n",
      "Index(['dataset_name', 'sequence_id', 'seq_len', 'excluded_term', 'gt_type',\n",
      "       'ground_truth', 'importance_sampling_est', 'hybrid_sampling_est',\n",
      "       'num_mc_samples', 'importance_model_iters', 'hybrid_model_iters',\n",
      "       'importance_est_variance', 'hybrid_est_variance', 'true_coverage',\n",
      "       'restricted_coverage', 'top_k', 'top_p', 'min_variance',\n",
      "       'min_var_reduction', 'num_beams', 'interp_func', 'hist_len',\n",
      "       'total_seq_len'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = flatten_experiments(['val_dl'],['shakespeare'],[(18,20),(17,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce843b1-19fd-4a08-bd5e-a76576456916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['importance_sampling']['metadata']['sub_estimates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a52b7fd-4659-494d-9ff3-13584d46e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('padhraic_extract_shakespeare_17-18_20.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193194d0-d619-4e39-ba5f-ee253a686bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
