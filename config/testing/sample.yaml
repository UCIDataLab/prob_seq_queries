# General args
seed: 1234321
dont_print_args: False
cuda: true
device_num: 3
train_data_pct: 0.9
val_data_pct: 0.05
seq_len: 100
do_test: false

# Model args
rnn_type: LSTM
hidden_size: 128
dropout: 0.3
num_layers: 2
masked_lm: false
checkpoint_path: models/shakespeare/
vocab_size: 68

# Data args
data_path: data/shakespeare_input.txt
batch_size: 1024

# Training args not needed here
# Evaluation arguments not needed here
# Sampling arguments are provided by the testing script explicitly
disable_tqdm: true

# Sampling args
estimate_type: search
# Bug here where if you increase this somehow the overall coverage goes down
proposal_func: uniform
interp_func: linear
top_k: 0
top_p: 0
num_beams: 0.8
num_mc_samples: 1000
total_seq_len: 30
hist_len: 20
excluded_terms: [1]


