# General args
seed: 1234321
dont_print_args: False
cuda: true
device_num: 3
train_data_pct: 0.9
val_data_pct: 0.05
seq_len: 100
do_test: false

# Model args
rnn_type: LSTM
hidden_size: 128
dropout: 0.3
num_layers: 2
masked_lm: false
checkpoint_path: models/shakespeare/
vocab_size: 68

# Data args
data_path: data/shakespeare_input.txt
batch_size: 128

# Training args not needed here
# Evaluation arguments not needed here
# Sampling arguments are provided by the testing script explicitly
disable_tqdm: false

# Sampling args
estimate_type: search
# Bug here where if you increase this somehow the overall coverage goes down
proposal_func: uniform
interp_func: linear
top_k: 100
top_p: 0.95
num_beams: 1.0
num_mc_samples: 1000
total_seq_len: 20
hist_len: 18
excluded: [1]


