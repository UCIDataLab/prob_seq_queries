# General args
seed: 1234321
dont_print_args: false
cuda: true
device_num: 5
train_data_pct: 0.9
val_data_pct: 0.05
seq_len: 100
do_test: false

# Model args
rnn_type: LSTM
hidden_size: 128
dropout: 0.3
num_layers: 2
masked_lm: false
checkpoint_path: models/shakespeare/
vocab_size: 68

# Data args
data_path: data/shakespeare_input.txt
batch_size: 1000

# Training args not needed here
# Evaluation arguments not needed here
# Sampling arguments are provided by the testing script explicitly
disable_tqdm: true

# Sampling args
sample_type: "mc_random"
# Bug here where if you increase this somehow the overall coverage goes down
beam_widths: 0.90
sample_args: {"coverage_type":"backoff"}
num_seqs: 100000
total_seq_lens: 20
hist_len: 18
excluded: [1]


